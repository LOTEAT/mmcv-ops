{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid 'ImportError: libc10.so: cannot open shared object file: No such file or directory'\n",
    "# we need to import torch\n",
    "import torch\n",
    "from mmcv_ops.utils import load_ext\n",
    "from mmcv_ops.roi_align import RoIAlign\n",
    "\n",
    "ext_module = load_ext('_ext', [\n",
    "    'roi_align_forward_cpu', 'roi_align_backward_cpu',\n",
    "    'roi_align_forward_cuda', 'roi_align_backward_cuda'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "# CPU Model\n",
    "roi_align_layer_cpu = RoIAlign((3, 3), 0.5, 0)\n",
    "# generate features randomly (bs, C, H, W)\n",
    "feats_cpu = torch.randn((1, 2, 10, 10), dtype=torch.float32, requires_grad=True)\n",
    "# create rois\n",
    "# 0 is batch idx\n",
    "# each roi is organised by (batch_idx, x1, y1, x2, y2)\n",
    "rois_cpu = torch.tensor([[0, 2, 2, 10, 10]], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats:\n",
      "\n",
      "tensor([[[[ 0.1257, -1.9619, -0.1382,  1.3187,  0.7176, -0.3532, -0.7202,\n",
      "            2.2736, -0.0474, -1.0741],\n",
      "          [-0.6901, -1.2496, -1.0259, -0.1270, -0.6580,  0.6004,  0.4073,\n",
      "           -1.3295,  1.0427,  1.6906],\n",
      "          [ 1.8739,  0.2285, -0.6754,  1.5749, -0.6862, -0.3564,  0.4297,\n",
      "            3.6265, -1.4205,  1.5400],\n",
      "          [ 0.5917,  0.0906,  0.0173,  1.8198, -0.4953, -1.2511,  0.2089,\n",
      "            0.7291, -0.7137, -0.4959],\n",
      "          [ 0.4790, -0.9269,  1.4868, -1.5470,  0.7609, -1.0193,  0.3684,\n",
      "           -1.2842,  1.3593, -0.5944],\n",
      "          [-0.4893,  1.0714,  2.5024, -1.5468, -0.0141,  0.3112, -1.8697,\n",
      "            0.4981,  0.0472, -0.7115],\n",
      "          [-1.1766, -0.8875, -0.4125,  0.3101, -1.6586, -0.2818, -0.2802,\n",
      "            0.1481, -0.8391,  0.3456],\n",
      "          [-0.4128,  2.3628, -0.3821, -0.1831,  0.6396, -1.0878, -0.8211,\n",
      "           -0.7375,  1.0809, -0.0933],\n",
      "          [-0.6507, -0.0699, -0.2249,  0.8217, -0.3815, -0.8674, -0.9957,\n",
      "            0.8582, -1.9554, -0.1454],\n",
      "          [ 1.1847, -2.1104,  0.7080, -1.7311, -0.0073,  2.1485, -0.1301,\n",
      "           -1.5706,  2.9056,  0.5584]],\n",
      "\n",
      "         [[ 0.7406,  0.4550, -0.6846, -0.5475,  0.0070, -2.7422,  0.4512,\n",
      "            0.3591,  1.1166, -0.3109],\n",
      "          [-0.1483, -0.9902,  1.4216,  0.9416,  0.0890, -0.2776, -1.6265,\n",
      "           -0.2094, -0.2782, -0.2820],\n",
      "          [ 0.2841, -1.3930, -1.3996,  0.7272, -0.1928, -0.5709,  0.1669,\n",
      "            0.4963, -0.8646,  1.7119],\n",
      "          [ 0.4952, -0.3521, -1.7116, -2.0975,  2.3099,  0.1321,  0.0316,\n",
      "            0.0923,  0.5400, -1.2272],\n",
      "          [-0.0613,  0.5138, -1.8742,  0.0394, -0.7064,  0.5698, -0.1799,\n",
      "            0.8742, -1.3822, -0.1887],\n",
      "          [-0.8440,  0.0811,  0.1241,  0.3643, -0.8182, -0.1058, -0.7744,\n",
      "            0.8981,  1.3824,  0.7384],\n",
      "          [ 0.3969, -0.1247, -0.0652, -0.4227, -0.3254,  0.3029, -0.0606,\n",
      "           -0.3576, -1.0483,  1.4911],\n",
      "          [ 0.0910, -0.5653, -0.4478, -0.9236, -0.1029,  0.0941, -0.1187,\n",
      "            0.7611, -0.8869,  0.4357],\n",
      "          [ 2.4716,  0.8133,  0.2591,  0.8121, -0.9119,  0.6665,  0.7378,\n",
      "            1.1552, -0.0721, -0.8734],\n",
      "          [ 0.3002,  1.3660,  0.5334, -0.2150, -1.2506, -0.7963, -0.4276,\n",
      "            0.5166, -2.2858,  0.7104]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# print feats\n",
    "print('feats:\\n')\n",
    "print(feats_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `forward` Function in RoIAlign\n",
    "The following is the forward function in RoIAlign Module.\n",
    "```python\n",
    "    def forward(self, input: torch.Tensor, rois: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: NCHW images\n",
    "            rois: Bx5 boxes. First column is the index into N.\\\n",
    "                The other 4 columns are xyxy.\n",
    "        \"\"\"\n",
    "        return roi_align(input, rois, self.output_size, self.spatial_scale,\n",
    "                         self.sampling_ratio, self.pool_mode, self.aligned)\n",
    "```\n",
    "We can find that it calls `roi_align` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `forward` Function in RoIAlignFunction\n",
    "Actually, `roi_align` is the `forward` function in `RoIAlignFunction`.\n",
    "\n",
    "```python\n",
    "    def forward(ctx: Any,\n",
    "                input: torch.Tensor,\n",
    "                rois: torch.Tensor,\n",
    "                output_size: int,\n",
    "                spatial_scale: float = 1.0,\n",
    "                sampling_ratio: int = 0,\n",
    "                pool_mode: str = 'avg',\n",
    "                aligned: bool = True) -> torch.Tensor:\n",
    "        device = input.device\n",
    "        # In our settings, ctx.output_size = (3, 3)\n",
    "        ctx.output_size = _pair(output_size)\n",
    "        # spatial_scale = 0.5\n",
    "        ctx.spatial_scale = spatial_scale\n",
    "        # If sampling_ratio > 0, we sample sampling_ratio * sampling_ratio points in each block\n",
    "        # else, the number of points we sample is based some rules\n",
    "        ctx.sampling_ratio = sampling_ratio\n",
    "        assert pool_mode in ('max', 'avg')\n",
    "        # Here pool mode is 1\n",
    "        ctx.pool_mode = 0 if pool_mode == 'max' else 1\n",
    "        # aligned = True\n",
    "        ctx.aligned = aligned\n",
    "        # input_shape = (1, 2, 10, 10)\n",
    "        ctx.input_shape = input.size()\n",
    "\n",
    "        assert rois.size(1) == 5, 'RoI must be (idx, x1, y1, x2, y2)!'\n",
    "        # output_shape = (1, 2, 3, 3)\n",
    "        output_shape = (rois.size(0), input.size(1), ctx.output_size[0],\n",
    "                        ctx.output_size[1])\n",
    "        # output\n",
    "        output = input.new_zeros(output_shape)\n",
    "        if ctx.pool_mode == 0:\n",
    "            argmax_y = input.new_zeros(output_shape)\n",
    "            argmax_x = input.new_zeros(output_shape)\n",
    "        else:\n",
    "            argmax_y = input.new_zeros(0)\n",
    "            argmax_x = input.new_zeros(0)\n",
    "        if device == 'cpu':\n",
    "            roi_align_forward = ext_module.roi_align_forward_cpu\n",
    "        else:\n",
    "            roi_align_forward = ext_module.roi_align_forward_cuda\n",
    "        roi_align_forward(input,\n",
    "                          rois,\n",
    "                          output,\n",
    "                          argmax_y,\n",
    "                          argmax_x,\n",
    "                          aligned_height=ctx.output_size[0],\n",
    "                          aligned_width=ctx.output_size[1],\n",
    "                          spatial_scale=ctx.spatial_scale,\n",
    "                          sampling_ratio=ctx.sampling_ratio,\n",
    "                          pool_mode=ctx.pool_mode,\n",
    "                          aligned=ctx.aligned)\n",
    "\n",
    "        ctx.save_for_backward(rois, argmax_y, argmax_x)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `roi_align_forward_cpu`\n",
    "Let's see `roi_align_forward_cpu` firstly.\n",
    "\n",
    "```cpp\n",
    "void roi_align_forward_cpu(Tensor input, Tensor rois, Tensor output,\n",
    "                                Tensor argmax_y, Tensor argmax_x,\n",
    "                                int aligned_height, int aligned_width,\n",
    "                                float spatial_scale, int sampling_ratio,\n",
    "                                int pool_mode, bool aligned) {\n",
    "  // output_size = 1 * 2 * 3 * 3 = 18\n",
    "  int output_size = output.numel();\n",
    "  // channels = 2\n",
    "  int channels = input.size(1);\n",
    "  // height = 10\n",
    "  int height = input.size(2);\n",
    "  // width = 10\n",
    "  int width = input.size(3);\n",
    "\n",
    "  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
    "      input.scalar_type(), \"ROIAlign_forward\", [&] {\n",
    "        ROIAlignForward<scalar_t>(\n",
    "            output_size, input.data_ptr<scalar_t>(), rois.data_ptr<scalar_t>(),\n",
    "            output.data_ptr<scalar_t>(), argmax_y.data_ptr<scalar_t>(),\n",
    "            argmax_x.data_ptr<scalar_t>(), aligned_height, aligned_width,\n",
    "            static_cast<scalar_t>(spatial_scale), sampling_ratio, pool_mode,\n",
    "            aligned, channels, height, width);\n",
    "      });\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ROIAlignForward\n",
    "In `roi_align_forward_cpu`, it calls `ROIAlignForward` function.\n",
    "\n",
    "```cpp\n",
    "template <typename T>\n",
    "void ROIAlignForward(const int nthreads, const T* input, const T* rois,\n",
    "                     T* output, T* argmax_y, T* argmax_x,\n",
    "                     const int pooled_height, const int pooled_width,\n",
    "                     const T spatial_scale, const int sampling_ratio,\n",
    "                     const int pool_mode,  // 0 - max pool, 1 - avg pool\n",
    "                     const bool aligned, const int channels, const int height,\n",
    "                     const int width) {\n",
    "  // nthreads are the output size, which is equal to \n",
    "  // nrois * channels * pooled_width * pooled_height\n",
    "  int n_rois = nthreads / channels / pooled_width / pooled_height;\n",
    "  // (n, c, ph, pw) is an element in the pooled output\n",
    "  // can be parallelized using omp\n",
    "  // #pragma omp parallel for num_threads(32)\n",
    "  // we iterate each roi\n",
    "  for (int n = 0; n < n_rois; n++) {\n",
    "    // Note that in C++, the Tensor type is organised by 1D array\n",
    "    // Therefore, we need to calculate the start index of output for n-th roi\n",
    "    // output is (n_rois, channels, pooled_width, pooled_height)\n",
    "    // For n-th roi, the start index should be `n * channels * pooled_width * pooled_height`\n",
    "    int index_n = n * channels * pooled_width * pooled_height;\n",
    "    // offset of n-th roi\n",
    "    const T* offset_rois = rois + n * 5;\n",
    "    // get batch idx\n",
    "    int roi_batch_ind = offset_rois[0];\n",
    "\n",
    "    // Do not use rounding; this implementation detail is critical\n",
    "    T offset = aligned ? (T)0.5 : (T)0.0;\n",
    "    // offset is just a trick, whether to align\n",
    "    // Remember that after convolution, the feature map is a downsample image\n",
    "    // And spatial_scale is the downsample rate\n",
    "    // roi_start_w = 0.5, roi_start_h = 0.5, roi_end_w = 4.5, roi_end_h = 4.5\n",
    "    T roi_start_w = offset_rois[1] * spatial_scale - offset;\n",
    "    T roi_start_h = offset_rois[2] * spatial_scale - offset;\n",
    "    T roi_end_w = offset_rois[3] * spatial_scale - offset;\n",
    "    T roi_end_h = offset_rois[4] * spatial_scale - offset;\n",
    "    // the width and height of roi in feature map\n",
    "    // roi_width = 4, roi_height = 4\n",
    "    T roi_width = roi_end_w - roi_start_w;\n",
    "    T roi_height = roi_end_h - roi_start_h;\n",
    "    if (aligned) {\n",
    "      AT_ASSERTM(roi_width >= 0 && roi_height >= 0,\n",
    "                 \"ROIs in ROIAlign cannot have non-negative size!\");\n",
    "    } else {  // for backward-compatibility only\n",
    "      roi_width = std::max(roi_width, (T)1.);\n",
    "      roi_height = std::max(roi_height, (T)1.);\n",
    "    }\n",
    "    // Notice that T is float\n",
    "    // Each output unit length corresponds to the length of the roi\n",
    "    // In our case, bin_size_h = 1.33, bin_size_w = 1.33\n",
    "    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);\n",
    "    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);\n",
    "\n",
    "    // We use roi_bin_grid to sample the grid and mimic integral\n",
    "    // roi_bin_grid_h = 2\n",
    "    // roi_bin_grid_w = 2\n",
    "    int roi_bin_grid_h = (sampling_ratio > 0)\n",
    "                             ? sampling_ratio\n",
    "                             : ceilf(roi_height / pooled_height);  // e.g., = 2\n",
    "    int roi_bin_grid_w =\n",
    "        (sampling_ratio > 0) ? sampling_ratio : ceilf(roi_width / pooled_width);\n",
    "\n",
    "    // When the grid is empty, output zeros == 0/1, instead of NaN.\n",
    "    // count = 4\n",
    "    const T count = std::max(roi_bin_grid_h * roi_bin_grid_w, 1);  // e.g. = 4\n",
    "\n",
    "    // we want to precalculate indices and weights shared by all channels,\n",
    "    // this is the key point of optimization\n",
    "    std::vector<PreCalc<T>> pre_calc(roi_bin_grid_h * roi_bin_grid_w *\n",
    "                                     pooled_width * pooled_height);\n",
    "    pre_calc_for_bilinear_interpolate(\n",
    "        height, width, pooled_height, pooled_width, roi_bin_grid_h,\n",
    "        roi_bin_grid_w, roi_start_h, roi_start_w, bin_size_h, bin_size_w,\n",
    "        roi_bin_grid_h, roi_bin_grid_w, pre_calc);\n",
    "    // before diving into more details, you should read section 5 & 6\n",
    "    // iteration for each channel\n",
    "    for (int c = 0; c < channels; c++) {\n",
    "      // index_n is the start index of n-th roi\n",
    "      // index_n_c is the start index of c-th channel of n-th roi\n",
    "      int index_n_c = index_n + c * pooled_width * pooled_height;\n",
    "      // ptr of feats\n",
    "      const T* offset_input =\n",
    "          input + (roi_batch_ind * channels + c) * height * width;\n",
    "      int pre_calc_index = 0;\n",
    "      // iteration for pooled_height\n",
    "      for (int ph = 0; ph < pooled_height; ph++) {\n",
    "        // iteration for pooled_width\n",
    "        for (int pw = 0; pw < pooled_width; pw++) {\n",
    "          // index of pooled pixel\n",
    "          int index = index_n_c + ph * pooled_width + pw;\n",
    "\n",
    "          T output_val = 0.;\n",
    "          T maxval = -10000;\n",
    "          T maxidx_y = -1.f, maxidx_x = -1.f;\n",
    "          for (int iy = 0; iy < roi_bin_grid_h; iy++) {\n",
    "            // y coordinate\n",
    "            const T y = roi_start_h + ph * bin_size_h +\n",
    "                        static_cast<T>(iy + .5f) * bin_size_h /\n",
    "                            static_cast<T>(roi_bin_grid_h);\n",
    "            for (int ix = 0; ix < roi_bin_grid_w; ix++) {\n",
    "              // x coordinate\n",
    "              const T x = roi_start_w + pw * bin_size_w +\n",
    "                          static_cast<T>(ix + .5f) * bin_size_w /\n",
    "                              static_cast<T>(roi_bin_grid_w);\n",
    "              PreCalc<T> pc = pre_calc[pre_calc_index];\n",
    "              T val = pc.w1 * offset_input[pc.pos1] +\n",
    "                      pc.w2 * offset_input[pc.pos2] +\n",
    "                      pc.w3 * offset_input[pc.pos3] +\n",
    "                      pc.w4 * offset_input[pc.pos4];\n",
    "              if (val > maxval) {\n",
    "                maxval = val;\n",
    "                maxidx_y = y;\n",
    "                maxidx_x = x;\n",
    "              }\n",
    "              output_val += val;\n",
    "              pre_calc_index += 1;\n",
    "            }\n",
    "          }\n",
    "          if (pool_mode == 0) {\n",
    "            // We do max pooling inside a bin\n",
    "            output[index] = maxval;\n",
    "            argmax_y[index] = maxidx_y;\n",
    "            argmax_x[index] = maxidx_x;\n",
    "          } else if (pool_mode == 1) {\n",
    "            // We do average (integral) pooling inside a bin\n",
    "            output[index] = output_val / count;\n",
    "          }  // if\n",
    "        }    // for pw\n",
    "      }      // for ph\n",
    "    }        // for c\n",
    "  }          // for n\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PreCalc\n",
    "One import data structure in `roi_align_forward_cpu` is `PreCalc`.\n",
    "```cpp\n",
    "template <typename T>\n",
    "struct PreCalc {\n",
    "  // positions and weights\n",
    "  int pos1;\n",
    "  int pos2;\n",
    "  int pos3;\n",
    "  int pos4;\n",
    "  T w1;\n",
    "  T w2;\n",
    "  T w3;\n",
    "  T w4;\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. `pre_calc_for_bilinear_interpolate` Function\n",
    "\n",
    "```cpp\n",
    "template <typename T>\n",
    "void pre_calc_for_bilinear_interpolate(\n",
    "    const int height, const int width, const int pooled_height,\n",
    "    const int pooled_width, const int iy_upper, const int ix_upper,\n",
    "    T roi_start_h, T roi_start_w, T bin_size_h, T bin_size_w,\n",
    "    int roi_bin_grid_h, int roi_bin_grid_w, std::vector<PreCalc<T>>& pre_calc) {\n",
    "  // height = 10, width = 10, pooled_height = 3, pooled_width = 3\n",
    "  // iy_upper = 2, ix_upper = 2, roi_start_h = 0.5, roi_start_w = 0.5\n",
    "  // bin_size_h = 1.33, bin_size_w = 1.33, roi_bin_grid_h = 2, roi_bin_grid_w = 2\n",
    "\n",
    "  // index counter\n",
    "  int pre_calc_index = 0;\n",
    "  // iteration for pooled_height\n",
    "  for (int ph = 0; ph < pooled_height; ph++) {\n",
    "    // iteration for pooled_width\n",
    "    for (int pw = 0; pw < pooled_width; pw++) {\n",
    "      // iteration for sampling points\n",
    "      // we will sample iy_upper point per column\n",
    "      for (int iy = 0; iy < iy_upper; iy++) {\n",
    "        // yy is the y coordinate of sampling point \n",
    "        // roi_start_h is y coordinate of top left corner of roi\n",
    "        // bin_size_h is gap in roi for per pooling pixel\n",
    "        // roi_start_h + ph * bin_size_h is the y coordinate of top left corner of roi for ph-th pooling block\n",
    "        // static_cast<T>(iy + .5f), move to center\n",
    "        // static_cast<T>(iy + .5f) / static_cast<T>(roi_bin_grid_h) percentage for iy-th sampling point in pooling area\n",
    "        // static_cast<T>(iy + .5f) * bin_size_h / static_cast<T>(roi_bin_grid_h) is the gap for top left corner of roi for ph-th pooling block\n",
    "        const T yy = roi_start_h + ph * bin_size_h +\n",
    "                     static_cast<T>(iy + .5f) * bin_size_h /\n",
    "                         static_cast<T>(roi_bin_grid_h);  // e.g., 0.5, 1.5\n",
    "        // iteration for sampling points\n",
    "        // we will sample ix_upper point per row\n",
    "        for (int ix = 0; ix < ix_upper; ix++) {\n",
    "          // xx is the x coordinate of sampling point\n",
    "          // the same with yy\n",
    "          const T xx = roi_start_w + pw * bin_size_w +\n",
    "                       static_cast<T>(ix + .5f) * bin_size_w /\n",
    "                           static_cast<T>(roi_bin_grid_w);\n",
    "\n",
    "          T x = xx;\n",
    "          T y = yy;\n",
    "          // deal with: inverse elements are out of feature map boundary\n",
    "          if (y < -1.0 || y > height || x < -1.0 || x > width) {\n",
    "            // empty\n",
    "            PreCalc<T> pc;\n",
    "            pc.pos1 = 0;\n",
    "            pc.pos2 = 0;\n",
    "            pc.pos3 = 0;\n",
    "            pc.pos4 = 0;\n",
    "            pc.w1 = 0;\n",
    "            pc.w2 = 0;\n",
    "            pc.w3 = 0;\n",
    "            pc.w4 = 0;\n",
    "            pre_calc[pre_calc_index] = pc;\n",
    "            pre_calc_index += 1;\n",
    "            continue;\n",
    "          }\n",
    "\n",
    "          if (y <= 0) {\n",
    "            y = 0;\n",
    "          }\n",
    "          if (x <= 0) {\n",
    "            x = 0;\n",
    "          }\n",
    "          // calculate for around points for bilinear interplotation\n",
    "          int y_low = (int)y;\n",
    "          int x_low = (int)x;\n",
    "          int y_high;\n",
    "          int x_high;\n",
    "\n",
    "          if (y_low >= height - 1) {\n",
    "            y_high = y_low = height - 1;\n",
    "            y = (T)y_low;\n",
    "          } else {\n",
    "            y_high = y_low + 1;\n",
    "          }\n",
    "\n",
    "          if (x_low >= width - 1) {\n",
    "            x_high = x_low = width - 1;\n",
    "            x = (T)x_low;\n",
    "          } else {\n",
    "            x_high = x_low + 1;\n",
    "          }\n",
    "          // y_low-y distance\n",
    "          T ly = y - y_low;\n",
    "          // x_low-x distance\n",
    "          T lx = x - x_low;\n",
    "          // y_high-y distance & x_high-x distance\n",
    "          T hy = 1. - ly, hx = 1. - lx;\n",
    "          // the weights of bilinear interplotation\n",
    "          // Remember in pixels, the denominator is 1\n",
    "          // w1, w2, w3, w4 should be (x2-x)(y2-y), (y2-y)(x-x1), (y-y1)(x2-x), (y-y1)(x-x1), respectively\n",
    "          T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;\n",
    "\n",
    "          // save weights and indices\n",
    "          PreCalc<T> pc;\n",
    "          pc.pos1 = y_low * width + x_low;\n",
    "          pc.pos2 = y_low * width + x_high;\n",
    "          pc.pos3 = y_high * width + x_low;\n",
    "          pc.pos4 = y_high * width + x_high;\n",
    "          pc.w1 = w1;\n",
    "          pc.w2 = w2;\n",
    "          pc.w3 = w3;\n",
    "          pc.w4 = w4;\n",
    "          pre_calc[pre_calc_index] = pc;\n",
    "\n",
    "          pre_calc_index += 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align roi cpu:\n",
      "\n",
      "tensor([[[[-8.4081e-01, -2.2266e-01, -2.3874e-01],\n",
      "          [ 1.2684e-01,  6.8415e-01, -3.6468e-02],\n",
      "          [-7.8885e-04,  2.4938e-01, -2.0245e-03]],\n",
      "\n",
      "         [[-5.0896e-01,  6.5235e-01,  1.5244e-01],\n",
      "          [-9.3811e-01, -1.1204e+00,  5.1612e-01],\n",
      "          [-2.4151e-01, -1.0674e+00, -5.7716e-02]]]],\n",
      "       grad_fn=<RoIAlignFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "align_roi_cpu = roi_align_layer_cpu(feats_cpu, rois_cpu)\n",
    "print('align roi cpu:\\n')\n",
    "print(align_roi_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "# GPU Model\n",
    "roi_align_layer_cuda = RoIAlign((3, 3), 0.5, 0).cuda()\n",
    "# generate features randomly (bs, C, H, W)\n",
    "feats_cuda = torch.randn((1, 2, 10, 10), dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "# create rois\n",
    "# 0 is batch idx\n",
    "# each roi is organised by (batch_idx, x1, y1, x2, y2)\n",
    "rois_cuda = torch.tensor([[0, 2, 2, 10, 10]], dtype=torch.float32, requires_grad=True, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats:\n",
      "\n",
      "tensor([[[[-0.0386, -0.6785,  0.7538, -1.1464,  0.3887,  0.4440, -0.2841,\n",
      "           -0.9236, -0.0969,  0.6881],\n",
      "          [ 0.3273, -0.3086,  1.2009, -0.0527,  0.2667, -0.0526,  1.8422,\n",
      "            0.2237, -0.6699, -1.1457],\n",
      "          [-0.7323,  0.3871, -1.2246,  0.5657, -0.5494,  0.6029,  1.1535,\n",
      "           -0.4653, -0.6445, -0.2874],\n",
      "          [-1.0605, -2.9157, -0.3628, -0.3066, -1.9257,  1.0506,  1.5880,\n",
      "           -0.1601,  0.5609, -0.5376],\n",
      "          [-1.6469, -1.0870, -1.1721, -0.8651, -0.8287, -0.2735, -0.3042,\n",
      "            0.3658, -0.7599,  0.4345],\n",
      "          [ 0.0796, -0.1492,  0.0307,  0.3946, -0.5278,  0.7277,  0.7529,\n",
      "            0.2595,  1.0894, -1.7413],\n",
      "          [-0.3076,  0.2513, -0.7556, -0.9918,  0.9225,  1.3222, -1.6966,\n",
      "           -1.6236, -0.3125, -1.1513],\n",
      "          [-1.2115,  1.6396,  0.3104,  0.9622, -0.7934, -0.1700,  0.1579,\n",
      "           -1.1424, -1.6679,  1.1257],\n",
      "          [ 0.4306, -1.0570, -0.0604, -0.6088,  1.6677,  0.2019, -0.0521,\n",
      "           -0.7722,  1.1427,  1.4866],\n",
      "          [ 2.0887,  0.0148,  0.7578, -0.4361,  1.1113,  0.8581, -0.1565,\n",
      "            0.8073,  0.0849, -1.6244]],\n",
      "\n",
      "         [[ 0.1872,  2.0385,  1.6927, -1.1786,  0.4881, -1.2939,  1.1030,\n",
      "            1.3240, -0.5688, -0.4245],\n",
      "          [-0.0231,  0.4034, -0.0256, -0.4126, -0.0793,  0.1981, -1.8776,\n",
      "            0.0360, -0.0646,  0.7047],\n",
      "          [-0.2176,  1.1034, -0.2807,  0.9804, -0.8033,  0.3551, -0.3646,\n",
      "            1.8768, -0.2468, -0.1760],\n",
      "          [ 1.0093,  1.6893, -1.4631, -1.8852, -0.3008,  1.1192, -0.0799,\n",
      "           -0.2449, -0.3972,  0.4616],\n",
      "          [ 0.6925,  0.9269,  0.6655,  2.2952,  0.0046, -1.1645,  0.4842,\n",
      "           -0.3458,  0.2071,  0.2979],\n",
      "          [ 0.7251,  1.9027,  0.6298,  0.0604,  1.2168, -1.3431,  1.0707,\n",
      "           -0.1196, -0.1103, -0.4055],\n",
      "          [ 0.9150, -0.6923, -0.1207,  1.9485, -1.9271, -1.6759,  2.0783,\n",
      "            2.9014, -1.1922, -1.5905],\n",
      "          [ 1.6147, -0.7579, -0.0449,  1.3984, -1.2579, -1.4976,  1.2211,\n",
      "            0.3334, -0.0383, -0.9678],\n",
      "          [-2.0330, -1.0022, -0.2030, -0.9580,  0.4345,  0.8659, -1.1697,\n",
      "           -0.0682, -2.6664, -1.5205],\n",
      "          [-1.9478, -2.0483, -0.8831, -0.0376,  0.8979, -0.7473, -0.4112,\n",
      "           -1.2248, -0.0715, -0.0858]]]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# print feats\n",
    "print('feats:\\n')\n",
    "print(feats_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align roi cuda:\n",
      "\n",
      "tensor([[[[ 0.0317,  0.2840,  0.0640],\n",
      "          [-1.1160, -0.3321, -0.7238],\n",
      "          [-1.3078, -0.7450, -0.8620]],\n",
      "\n",
      "         [[ 0.4854, -0.0372, -0.1647],\n",
      "          [ 0.7459, -0.6621, -0.4197],\n",
      "          [ 0.8964,  0.5971,  0.2348]]]], device='cuda:0',\n",
      "       grad_fn=<RoIAlignFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "align_roi_cuda = roi_align_layer_cuda(feats_cuda, rois_cuda)\n",
    "print('align roi cuda:\\n')\n",
    "print(align_roi_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. roi_align_forward_cuda\n",
    "\n",
    "Let's see `roi_align_forward_cuda` then.\n",
    "\n",
    "```cpp\n",
    "void roi_align_forward_cuda(Tensor input, Tensor rois, Tensor output,\n",
    "                                       Tensor argmax_y, Tensor argmax_x,\n",
    "                                       int aligned_height, int aligned_width,\n",
    "                                       float spatial_scale, int sampling_ratio,\n",
    "                                       int pool_mode, bool aligned) {\n",
    "  int output_size = output.numel();\n",
    "  int channels = input.size(1);\n",
    "  int height = input.size(2);\n",
    "  int width = input.size(3);\n",
    "\n",
    "  at::cuda::CUDAGuard device_guard(input.device());\n",
    "  cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n",
    "  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
    "      input.scalar_type(), \"roi_align_forward_cuda_kernel\", [&] {\n",
    "        roi_align_forward_cuda_kernel<scalar_t>\n",
    "            <<<GET_BLOCKS(output_size), THREADS_PER_BLOCK, 0, stream>>>(\n",
    "                output_size, input.data_ptr<scalar_t>(),\n",
    "                rois.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(),\n",
    "                argmax_y.data_ptr<scalar_t>(), argmax_x.data_ptr<scalar_t>(),\n",
    "                aligned_height, aligned_width,\n",
    "                static_cast<scalar_t>(spatial_scale), sampling_ratio, pool_mode,\n",
    "                aligned, channels, height, width);\n",
    "      });\n",
    "\n",
    "  AT_CUDA_CHECK(cudaGetLastError());\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. roi_align_forward_cuda_kernel\n",
    "Actually, we can find that `roi_align_forward_cuda_kernel` is almost the same with `roi_align_forward_cpu_kernel`.\n",
    "This implementation is multi-thread and is based on the number of RoIs.\n",
    "\n",
    "```cpp\n",
    "template <typename T>\n",
    "__global__ void roi_align_forward_cuda_kernel(\n",
    "    const int nthreads, const T* input, const T* rois, T* output, T* argmax_y,\n",
    "    T* argmax_x, const int pooled_height, const int pooled_width,\n",
    "    const T spatial_scale, const int sampling_ratio,\n",
    "    const int pool_mode,  // 0 - max pool, 1 - avg pool\n",
    "    const bool aligned, const int channels, const int height, const int width) {\n",
    "  CUDA_1D_KERNEL_LOOP(index, nthreads) {\n",
    "    // (n, c, ph, pw) is an element in the pooled output\n",
    "    int pw = index % pooled_width;\n",
    "    int ph = (index / pooled_width) % pooled_height;\n",
    "    int c = (index / pooled_width / pooled_height) % channels;\n",
    "    int n = index / pooled_width / pooled_height / channels;\n",
    "\n",
    "    const T* offset_rois = rois + n * 5;\n",
    "    int roi_batch_ind = offset_rois[0];\n",
    "\n",
    "    // Do not using rounding; this implementation detail is critical\n",
    "    T offset = aligned ? (T)0.5 : (T)0.0;\n",
    "    T roi_start_w = offset_rois[1] * spatial_scale - offset;\n",
    "    T roi_start_h = offset_rois[2] * spatial_scale - offset;\n",
    "    T roi_end_w = offset_rois[3] * spatial_scale - offset;\n",
    "    T roi_end_h = offset_rois[4] * spatial_scale - offset;\n",
    "\n",
    "    T roi_width = roi_end_w - roi_start_w;\n",
    "    T roi_height = roi_end_h - roi_start_h;\n",
    "    if (!aligned) {  // for backward-compatibility only\n",
    "      roi_width = max(roi_width, (T)1.);\n",
    "      roi_height = max(roi_height, (T)1.);\n",
    "    }\n",
    "\n",
    "    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);\n",
    "    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);\n",
    "\n",
    "    const T* offset_input =\n",
    "        input + (roi_batch_ind * channels + c) * height * width;\n",
    "\n",
    "    // We use roi_bin_grid to sample the grid and mimic integral\n",
    "    int roi_bin_grid_h =\n",
    "        (sampling_ratio > 0)\n",
    "            ? sampling_ratio\n",
    "            : static_cast<int>(ceilf(roi_height / pooled_height));\n",
    "    int roi_bin_grid_w =\n",
    "        (sampling_ratio > 0)\n",
    "            ? sampling_ratio\n",
    "            : static_cast<int>(ceilf(roi_width / pooled_width));\n",
    "\n",
    "    if (pool_mode == 0) {\n",
    "      // We do max pooling inside a bin\n",
    "      T maxval = -FLT_MAX;\n",
    "      T maxidx_y = -1.f, maxidx_x = -1.f;\n",
    "      for (int iy = 0; iy < roi_bin_grid_h; iy++) {\n",
    "        const T y = roi_start_h + ph * bin_size_h +\n",
    "                    static_cast<T>(iy + .5f) * bin_size_h /\n",
    "                        static_cast<T>(roi_bin_grid_h);\n",
    "        for (int ix = 0; ix < roi_bin_grid_w; ix++) {\n",
    "          const T x = roi_start_w + pw * bin_size_w +\n",
    "                      static_cast<T>(ix + .5f) * bin_size_w /\n",
    "                          static_cast<T>(roi_bin_grid_w);\n",
    "          T val =\n",
    "              bilinear_interpolate(offset_input, height, width, y, x, index);\n",
    "          if (val > maxval) {\n",
    "            maxval = val;\n",
    "            maxidx_y = y;\n",
    "            maxidx_x = x;\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "      output[index] = maxval;\n",
    "      argmax_y[index] = maxidx_y;\n",
    "      argmax_x[index] = maxidx_x;\n",
    "    } else if (pool_mode == 1) {\n",
    "      // We do average pooling inside a bin\n",
    "      const T count = max(roi_bin_grid_h * roi_bin_grid_w, 1);\n",
    "      T output_val = 0.;\n",
    "      for (int iy = 0; iy < roi_bin_grid_h; iy++) {\n",
    "        const T y = roi_start_h + ph * bin_size_h +\n",
    "                    static_cast<T>(iy + .5f) * bin_size_h /\n",
    "                        static_cast<T>(roi_bin_grid_h);\n",
    "        for (int ix = 0; ix < roi_bin_grid_w; ix++) {\n",
    "          const T x = roi_start_w + pw * bin_size_w +\n",
    "                      static_cast<T>(ix + .5f) * bin_size_w /\n",
    "                          static_cast<T>(roi_bin_grid_w);\n",
    "          T val =\n",
    "              bilinear_interpolate(offset_input, height, width, y, x, index);\n",
    "          output_val += val;\n",
    "        }\n",
    "      }\n",
    "      output[index] = output_val / count;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. roi_align_backward_cpu\n",
    "\n",
    "```cpp\n",
    "void roi_align_backward_cpu(Tensor grad_output, Tensor rois,\n",
    "                                 Tensor argmax_y, Tensor argmax_x,\n",
    "                                 Tensor grad_input, int aligned_height,\n",
    "                                 int aligned_width, float spatial_scale,\n",
    "                                 int sampling_ratio, int pool_mode,\n",
    "                                 bool aligned) {\n",
    "  int output_size = grad_output.numel();\n",
    "  int channels = grad_input.size(1);\n",
    "  int height = grad_input.size(2);\n",
    "  int width = grad_input.size(3);\n",
    "\n",
    "  // get stride values to ensure indexing into gradients is correct.\n",
    "  int n_stride = grad_output.stride(0);\n",
    "  int c_stride = grad_output.stride(1);\n",
    "  int h_stride = grad_output.stride(2);\n",
    "  int w_stride = grad_output.stride(3);\n",
    "\n",
    "  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
    "      grad_output.scalar_type(), \"ROIAlign_backward\", [&] {\n",
    "        ROIAlignBackward<scalar_t>(\n",
    "            output_size, grad_output.data_ptr<scalar_t>(),\n",
    "            rois.data_ptr<scalar_t>(), argmax_y.data_ptr<scalar_t>(),\n",
    "            argmax_x.data_ptr<scalar_t>(), grad_input.data_ptr<scalar_t>(),\n",
    "            aligned_height, aligned_width, static_cast<scalar_t>(spatial_scale),\n",
    "            sampling_ratio, pool_mode, aligned, channels, height, width,\n",
    "            n_stride, c_stride, h_stride, w_stride);\n",
    "      });\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. `backward` Function in `RoIAlignFunction`\n",
    "We also should know the backward process. Note that the `grad_output` should be a tensor whose dimension is (n_rois, C, output_size, output_size). In our setting, the dimension is (1, 2, 3, 3).\n",
    "\n",
    "```python\n",
    "    def backward(ctx: Any, grad_output: torch.Tensor) -> tuple:\n",
    "        rois, argmax_y, argmax_x = ctx.saved_tensors\n",
    "        device = rois.device\n",
    "        grad_input = grad_output.new_zeros(ctx.input_shape)\n",
    "        # complex head architecture may cause grad_output uncontiguous.\n",
    "        grad_output = grad_output.contiguous()\n",
    "        if device == 'cpu':\n",
    "            roi_align_backward = ext_module.roi_align_backward_cpu\n",
    "        else:\n",
    "            roi_align_backward = ext_module.roi_align_backward_cuda\n",
    "        roi_align_backward(grad_output,\n",
    "                           rois,\n",
    "                           argmax_y,\n",
    "                           argmax_x,\n",
    "                           grad_input,\n",
    "                           aligned_height=ctx.output_size[0],\n",
    "                           aligned_width=ctx.output_size[1],\n",
    "                           spatial_scale=ctx.spatial_scale,\n",
    "                           sampling_ratio=ctx.sampling_ratio,\n",
    "                           pool_mode=ctx.pool_mode,\n",
    "                           aligned=ctx.aligned)\n",
    "        return grad_input, None, None, None, None, None, None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. roi_align_backward_cpu\n",
    "This is the entrance to back propagation.\n",
    "\n",
    "```cpp\n",
    "void roi_align_backward_cpu(Tensor grad_output, Tensor rois,\n",
    "                                 Tensor argmax_y, Tensor argmax_x,\n",
    "                                 Tensor grad_input, int aligned_height,\n",
    "                                 int aligned_width, float spatial_scale,\n",
    "                                 int sampling_ratio, int pool_mode,\n",
    "                                 bool aligned) {\n",
    "  int output_size = grad_output.numel();\n",
    "  int channels = grad_input.size(1);\n",
    "  int height = grad_input.size(2);\n",
    "  int width = grad_input.size(3);\n",
    "\n",
    "  // get stride values to ensure indexing into gradients is correct.\n",
    "  int n_stride = grad_output.stride(0);\n",
    "  int c_stride = grad_output.stride(1);\n",
    "  int h_stride = grad_output.stride(2);\n",
    "  int w_stride = grad_output.stride(3);\n",
    "\n",
    "  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
    "      grad_output.scalar_type(), \"ROIAlign_backward\", [&] {\n",
    "        ROIAlignBackward<scalar_t>(\n",
    "            output_size, grad_output.data_ptr<scalar_t>(),\n",
    "            rois.data_ptr<scalar_t>(), argmax_y.data_ptr<scalar_t>(),\n",
    "            argmax_x.data_ptr<scalar_t>(), grad_input.data_ptr<scalar_t>(),\n",
    "            aligned_height, aligned_width, static_cast<scalar_t>(spatial_scale),\n",
    "            sampling_ratio, pool_mode, aligned, channels, height, width,\n",
    "            n_stride, c_stride, h_stride, w_stride);\n",
    "      });\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. ROIAlignBackward\n",
    "\n",
    "It is easy for you to derive the Back Propagation formula.\n",
    "\n",
    "```cpp\n",
    "template <typename T>\n",
    "void ROIAlignBackward(const int nthreads, const T* grad_output, const T* rois,\n",
    "                      const T* argmax_y, const T* argmax_x, T* grad_input,\n",
    "                      const int pooled_height, const int pooled_width,\n",
    "                      const T spatial_scale, const int sampling_ratio,\n",
    "                      const int pool_mode,  // 0 - max pool, 1 - avg pool\n",
    "                      const bool aligned, const int channels, const int height,\n",
    "                      const int width, const int n_stride, const int c_stride,\n",
    "                      const int h_stride, const int w_stride) {\n",
    "  // nthreads = 1 * 2 * 10 * 10 = 200\n",
    "  // For now, grad_out is a grad matrix whose dimension is (1, 2, 3, 3)\n",
    "  // the dimension of rois is (1, 5)\n",
    "  // argmax_y, argmax_x is memorized in forward step\n",
    "  // grad_input is a zero matrix whose dimension is (1, 2, 10, 10)\n",
    "  // pooled_height = 3\n",
    "  // pooled_width = 3\n",
    "  // spatial_scale = 0.5\n",
    "  // sampling_ratio = 0\n",
    "  // pool_mode = 0\n",
    "  // aligned = True\n",
    "  // channels = 2\n",
    "  // height = 10\n",
    "  // width = 10\n",
    "  // n_stride = 18\n",
    "  // c_stride = 9\n",
    "  // h_stride = 3\n",
    "  // w_stride = 1\n",
    "  for (int index = 0; index < nthreads; index++) {\n",
    "    // (n, c, ph, pw) is an element in the pooled output\n",
    "    int pw = index % pooled_width;\n",
    "    int ph = (index / pooled_width) % pooled_height;\n",
    "    int c = (index / pooled_width / pooled_height) % channels;\n",
    "    int n = index / pooled_width / pooled_height / channels;\n",
    "    // for n-th roi\n",
    "    const T* offset_rois = rois + n * 5;\n",
    "    // get batch idx\n",
    "    int roi_batch_ind = offset_rois[0];\n",
    "\n",
    "    // Do not use rounding; this implementation detail is critical\n",
    "    T offset = aligned ? (T)0.5 : (T)0.0;\n",
    "    // calculate the roi coordinate, width and height\n",
    "    T roi_start_w = offset_rois[1] * spatial_scale - offset;\n",
    "    T roi_start_h = offset_rois[2] * spatial_scale - offset;\n",
    "    T roi_end_w = offset_rois[3] * spatial_scale - offset;\n",
    "    T roi_end_h = offset_rois[4] * spatial_scale - offset;\n",
    "\n",
    "    T roi_width = roi_end_w - roi_start_w;\n",
    "    T roi_height = roi_end_h - roi_start_h;\n",
    "    if (aligned) {\n",
    "      AT_ASSERTM(roi_width >= 0 && roi_height >= 0,\n",
    "                 \"ROIs in ROIAlign do not have non-negative size!\");\n",
    "    } else {  // for backward-compatibility only\n",
    "      roi_width = std::max(roi_width, (T)1.);\n",
    "      roi_height = std::max(roi_height, (T)1.);\n",
    "    }\n",
    "    // Notice that T is float\n",
    "    // Each output unit length corresponds to the length of the roi\n",
    "    // In our case, bin_size_h = 1.33, bin_size_w = 1.33\n",
    "    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);\n",
    "    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);\n",
    "\n",
    "    // get n-th feature map's grad\n",
    "    T* offset_grad_input =\n",
    "        grad_input + ((roi_batch_ind * channels + c) * height * width);\n",
    "    // get (n, c, ph, pw) element in grad_output\n",
    "    int output_offset = n * n_stride + c * c_stride;\n",
    "    const T* offset_grad_output = grad_output + output_offset;\n",
    "    const T grad_output_this_bin =\n",
    "        offset_grad_output[ph * h_stride + pw * w_stride];\n",
    "\n",
    "    if (pool_mode == 0) {\n",
    "      // We do max pooling inside a bin\n",
    "      // get max point\n",
    "      T y = argmax_y[index], x = argmax_x[index];\n",
    "      if (y != -1.f) {\n",
    "        T w1, w2, w3, w4;\n",
    "        int x_low, x_high, y_low, y_high;\n",
    "        // Before diving into more details, you should read Sec. 13,\n",
    "        bilinear_interpolate_gradient(height, width, y, x, w1, w2, w3, w4,\n",
    "                                      x_low, x_high, y_low, y_high, index);\n",
    "        // w * grad_output\n",
    "        T g1 = grad_output_this_bin * w1;\n",
    "        T g2 = grad_output_this_bin * w2;\n",
    "        T g3 = grad_output_this_bin * w3;\n",
    "        T g4 = grad_output_this_bin * w4;\n",
    "\n",
    "        if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {\n",
    "          // atomic add is not needed for now since it is single threaded\n",
    "          add(offset_grad_input + y_low * width + x_low, static_cast<T>(g1));\n",
    "          add(offset_grad_input + y_low * width + x_high, static_cast<T>(g2));\n",
    "          add(offset_grad_input + y_high * width + x_low, static_cast<T>(g3));\n",
    "          add(offset_grad_input + y_high * width + x_high, static_cast<T>(g4));\n",
    "        }  // if\n",
    "      }    // mode\n",
    "    } else if (pool_mode == 1) {\n",
    "      // We do average (integral) pooling inside a bin\n",
    "      // We use roi_bin_grid to sample the grid and mimic integral\n",
    "      int roi_bin_grid_h =\n",
    "          (sampling_ratio > 0)\n",
    "              ? sampling_ratio\n",
    "              : ceilf(roi_height / pooled_height);  // e.g., = 2\n",
    "      int roi_bin_grid_w = (sampling_ratio > 0)\n",
    "                               ? sampling_ratio\n",
    "                               : ceilf(roi_width / pooled_width);\n",
    "\n",
    "      const T count = roi_bin_grid_h * roi_bin_grid_w;  // e.g. = 4\n",
    "      for (int iy = 0; iy < roi_bin_grid_h; iy++) {\n",
    "        const T y = roi_start_h + ph * bin_size_h +\n",
    "                    static_cast<T>(iy + .5f) * bin_size_h /\n",
    "                        static_cast<T>(roi_bin_grid_h);  // e.g., 0.5, 1.5\n",
    "        for (int ix = 0; ix < roi_bin_grid_w; ix++) {\n",
    "          const T x = roi_start_w + pw * bin_size_w +\n",
    "                      static_cast<T>(ix + .5f) * bin_size_w /\n",
    "                          static_cast<T>(roi_bin_grid_w);\n",
    "\n",
    "          T w1, w2, w3, w4;\n",
    "          int x_low, x_high, y_low, y_high;\n",
    "\n",
    "          bilinear_interpolate_gradient(height, width, y, x, w1, w2, w3, w4,\n",
    "                                        x_low, x_high, y_low, y_high, index);\n",
    "\n",
    "          T g1 = grad_output_this_bin * w1 / count;\n",
    "          T g2 = grad_output_this_bin * w2 / count;\n",
    "          T g3 = grad_output_this_bin * w3 / count;\n",
    "          T g4 = grad_output_this_bin * w4 / count;\n",
    "\n",
    "          if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {\n",
    "            // atomic add is not needed for now since it is single threaded\n",
    "            add(offset_grad_input + y_low * width + x_low, static_cast<T>(g1));\n",
    "            add(offset_grad_input + y_low * width + x_high, static_cast<T>(g2));\n",
    "            add(offset_grad_input + y_high * width + x_low, static_cast<T>(g3));\n",
    "            add(offset_grad_input + y_high * width + x_high,\n",
    "                static_cast<T>(g4));\n",
    "          }  // if\n",
    "        }    // ix\n",
    "      }      // iy\n",
    "    }        // mode\n",
    "  }          // for\n",
    "}  // ROIAlignBackward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 13. bilinear_interpolate_gradient\n",
    "This function helps us to calculate the gradient of bilinear interpolatation.\n",
    "\n",
    "```cpp\n",
    "template <typename T>\n",
    "void bilinear_interpolate_gradient(const int height, const int width, T y, T x,\n",
    "                                   T& w1, T& w2, T& w3, T& w4, int& x_low,\n",
    "                                   int& x_high, int& y_low, int& y_high,\n",
    "                                   const int index /* index for debug only*/) {\n",
    "  // deal with cases that inverse elements are out of feature map boundary\n",
    "  if (y < -1.0 || y > height || x < -1.0 || x > width) {\n",
    "    // empty\n",
    "    w1 = w2 = w3 = w4 = 0.;\n",
    "    x_low = x_high = y_low = y_high = -1;\n",
    "    return;\n",
    "  }\n",
    "\n",
    "  if (y <= 0) y = 0;\n",
    "  if (x <= 0) x = 0;\n",
    "\n",
    "  y_low = (int)y;\n",
    "  x_low = (int)x;\n",
    "\n",
    "  if (y_low >= height - 1) {\n",
    "    y_high = y_low = height - 1;\n",
    "    y = (T)y_low;\n",
    "  } else {\n",
    "    y_high = y_low + 1;\n",
    "  }\n",
    "\n",
    "  if (x_low >= width - 1) {\n",
    "    x_high = x_low = width - 1;\n",
    "    x = (T)x_low;\n",
    "  } else {\n",
    "    x_high = x_low + 1;\n",
    "  }\n",
    "\n",
    "  T ly = y - y_low;\n",
    "  T lx = x - x_low;\n",
    "  T hy = 1. - ly, hx = 1. - lx;\n",
    "\n",
    "  // reference in forward\n",
    "  // T v1 = input[y_low * width + x_low];\n",
    "  // T v2 = input[y_low * width + x_high];\n",
    "  // T v3 = input[y_high * width + x_low];\n",
    "  // T v4 = input[y_high * width + x_high];\n",
    "  // T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n",
    "\n",
    "  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;\n",
    "\n",
    "  return;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. add\n",
    "Add function.\n",
    "```cpp\n",
    "template <class T>\n",
    "inline void add(T* address, const T& val) {\n",
    "  *address += val;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "align_roi_sum_cpu = align_roi_cpu.sum()\n",
    "align_roi_sum_cpu.backward()\n",
    "print(feats_cpu.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. roi_align_backward_cuda\n",
    "\n",
    "```cpp\n",
    "void roi_align_backward_cuda(Tensor grad_output, Tensor rois,\n",
    "                                        Tensor argmax_y, Tensor argmax_x,\n",
    "                                        Tensor grad_input, int aligned_height,\n",
    "                                        int aligned_width, float spatial_scale,\n",
    "                                        int sampling_ratio, int pool_mode,\n",
    "                                        bool aligned) {\n",
    "  int output_size = grad_output.numel();\n",
    "  int channels = grad_input.size(1);\n",
    "  int height = grad_input.size(2);\n",
    "  int width = grad_input.size(3);\n",
    "\n",
    "  at::cuda::CUDAGuard device_guard(grad_output.device());\n",
    "  cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n",
    "  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
    "      grad_output.scalar_type(), \"roi_align_backward_cuda_kernel\", [&] {\n",
    "        roi_align_backward_cuda_kernel<scalar_t>\n",
    "            <<<GET_BLOCKS(output_size), THREADS_PER_BLOCK, 0, stream>>>(\n",
    "                output_size, grad_output.data_ptr<scalar_t>(),\n",
    "                rois.data_ptr<scalar_t>(), argmax_y.data_ptr<scalar_t>(),\n",
    "                argmax_x.data_ptr<scalar_t>(), grad_input.data_ptr<scalar_t>(),\n",
    "                aligned_height, aligned_width,\n",
    "                static_cast<scalar_t>(spatial_scale), sampling_ratio, pool_mode,\n",
    "                aligned, channels, height, width);\n",
    "      });\n",
    "\n",
    "  AT_CUDA_CHECK(cudaGetLastError());\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. roi_align_backward_cuda_kernel\n",
    "It is almost the same with `roi_align_backward_cpu_kernel`. Note that in cuda kernel we should use `atomicAdd`.\n",
    "\n",
    "```cpp\n",
    "template <typename T>\n",
    "__global__ void roi_align_backward_cuda_kernel(\n",
    "    const int nthreads, const T* grad_output, const T* rois, const T* argmax_y,\n",
    "    const T* argmax_x, T* grad_input, const int pooled_height,\n",
    "    const int pooled_width, const T spatial_scale, const int sampling_ratio,\n",
    "    const int pool_mode,  // 0 - max pool, 1 - avg pool\n",
    "    const bool aligned, const int channels, const int height, const int width) {\n",
    "  CUDA_1D_KERNEL_LOOP(index, nthreads) {\n",
    "    // (n, c, ph, pw) is an element in the pooled output\n",
    "    int pw = index % pooled_width;\n",
    "    int ph = (index / pooled_width) % pooled_height;\n",
    "    int c = (index / pooled_width / pooled_height) % channels;\n",
    "    int n = index / pooled_width / pooled_height / channels;\n",
    "\n",
    "    const T grad_output_this_bin = grad_output[index];\n",
    "\n",
    "    const T* offset_rois = rois + n * 5;\n",
    "    int roi_batch_ind = offset_rois[0];\n",
    "    T* offset_grad_input =\n",
    "        grad_input + ((roi_batch_ind * channels + c) * height * width);\n",
    "\n",
    "    if (pool_mode == 0) {\n",
    "      T y = argmax_y[index], x = argmax_x[index];\n",
    "      if (y != -1.f) {\n",
    "        T w1, w2, w3, w4;\n",
    "        int x_low, x_high, y_low, y_high;\n",
    "        bilinear_interpolate_gradient(height, width, y, x, w1, w2, w3, w4,\n",
    "                                      x_low, x_high, y_low, y_high, index);\n",
    "\n",
    "        if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {\n",
    "          atomicAdd(offset_grad_input + y_low * width + x_low,\n",
    "                    grad_output_this_bin * w1);\n",
    "          atomicAdd(offset_grad_input + y_low * width + x_high,\n",
    "                    grad_output_this_bin * w2);\n",
    "          atomicAdd(offset_grad_input + y_high * width + x_low,\n",
    "                    grad_output_this_bin * w3);\n",
    "          atomicAdd(offset_grad_input + y_high * width + x_high,\n",
    "                    grad_output_this_bin * w4);\n",
    "        }\n",
    "      }\n",
    "    } else if (pool_mode == 1) {\n",
    "      // Do not using rounding; this implementation detail is critical\n",
    "      T offset = aligned ? (T)0.5 : (T)0.0;\n",
    "      T roi_start_w = offset_rois[1] * spatial_scale - offset;\n",
    "      T roi_start_h = offset_rois[2] * spatial_scale - offset;\n",
    "      T roi_end_w = offset_rois[3] * spatial_scale - offset;\n",
    "      T roi_end_h = offset_rois[4] * spatial_scale - offset;\n",
    "\n",
    "      T roi_width = roi_end_w - roi_start_w;\n",
    "      T roi_height = roi_end_h - roi_start_h;\n",
    "      if (!aligned) {  // for backward-compatibility only\n",
    "        roi_width = max(roi_width, (T)1.);\n",
    "        roi_height = max(roi_height, (T)1.);\n",
    "      }\n",
    "\n",
    "      T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);\n",
    "      T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);\n",
    "\n",
    "      // We use roi_bin_grid to sample the grid and mimic integral\n",
    "      int roi_bin_grid_h =\n",
    "          (sampling_ratio > 0)\n",
    "              ? sampling_ratio\n",
    "              : static_cast<int>(ceilf(roi_height / pooled_height));\n",
    "      int roi_bin_grid_w =\n",
    "          (sampling_ratio > 0)\n",
    "              ? sampling_ratio\n",
    "              : static_cast<int>(ceilf(roi_width / pooled_width));\n",
    "\n",
    "      // We do average (integral) pooling inside a bin\n",
    "      const T count = roi_bin_grid_h * roi_bin_grid_w;  // e.g. = 4\n",
    "\n",
    "      for (int iy = 0; iy < roi_bin_grid_h; iy++) {\n",
    "        const T y = roi_start_h + ph * bin_size_h +\n",
    "                    static_cast<T>(iy + .5f) * bin_size_h /\n",
    "                        static_cast<T>(roi_bin_grid_h);\n",
    "        for (int ix = 0; ix < roi_bin_grid_w; ix++) {\n",
    "          const T x = roi_start_w + pw * bin_size_w +\n",
    "                      static_cast<T>(ix + .5f) * bin_size_w /\n",
    "                          static_cast<T>(roi_bin_grid_w);\n",
    "\n",
    "          T w1, w2, w3, w4;\n",
    "          int x_low, x_high, y_low, y_high;\n",
    "          bilinear_interpolate_gradient(height, width, y, x, w1, w2, w3, w4,\n",
    "                                        x_low, x_high, y_low, y_high, index);\n",
    "\n",
    "          if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {\n",
    "            atomicAdd(offset_grad_input + y_low * width + x_low,\n",
    "                      grad_output_this_bin * w1 / count);\n",
    "            atomicAdd(offset_grad_input + y_low * width + x_high,\n",
    "                      grad_output_this_bin * w2 / count);\n",
    "            atomicAdd(offset_grad_input + y_high * width + x_low,\n",
    "                      grad_output_this_bin * w3 / count);\n",
    "            atomicAdd(offset_grad_input + y_high * width + x_high,\n",
    "                      grad_output_this_bin * w4 / count);\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0625, 0.5000, 0.5625, 0.5625, 0.5000, 0.0625, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0556, 0.4444, 0.5000, 0.5000, 0.4444, 0.0556, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0069, 0.0556, 0.0625, 0.0625, 0.0556, 0.0069, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "align_roi_sum_cuda = align_roi_cuda.sum()\n",
    "align_roi_sum_cuda.backward()\n",
    "print(feats_cuda.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
